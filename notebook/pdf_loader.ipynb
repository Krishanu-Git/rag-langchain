{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "466e9423",
   "metadata": {},
   "source": [
    "### RAG Pipeline - Data ingestion to vector db pipeline\n",
    "#### 1. Data Ingestion\n",
    "#### 2. Data Parsing - chunks\n",
    "#### 3. Embeddings - text/vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c53a6f5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c0b63e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import PyMuPDFLoader, DirectoryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7ed7e815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 3 PDF files to process\n",
      "\n",
      "Processing Assigntment 2.pdf\n",
      " - Loaded document with 139 characters\n",
      " - Loaded document with 1669 characters\n",
      " - Loaded document with 810 characters\n",
      " pages loaded: 3\n",
      "\n",
      "Processing Assignment 1.pdf\n",
      " - Loaded document with 139 characters\n",
      " - Loaded document with 1928 characters\n",
      " - Loaded document with 1658 characters\n",
      " - Loaded document with 1712 characters\n",
      " - Loaded document with 678 characters\n",
      " pages loaded: 5\n",
      "\n",
      "Processing assignment.pdf\n",
      " - Loaded document with 2093 characters\n",
      " pages loaded: 1\n"
     ]
    }
   ],
   "source": [
    "def process_all_pdfs(pdf_directory):\n",
    "    \"\"\"\n",
    "    process all the pdfs files in a directory\n",
    "    \"\"\"\n",
    "    all_documents = []\n",
    "    pdf_dir = Path(pdf_directory)\n",
    "\n",
    "    # recursively find the files\n",
    "    pdf_files = list(pdf_dir.glob(\"**/*.pdf\"))\n",
    "\n",
    "    print(f\"found {len(pdf_files)} PDF files to process\")\n",
    "\n",
    "    for pdf_file in pdf_files:\n",
    "        print(f\"\\nProcessing {pdf_file.name}\")\n",
    "        try:\n",
    "            loader = PyMuPDFLoader(str(pdf_file))\n",
    "            documents = loader.load()\n",
    "            for doc in documents:\n",
    "                print(f\" - Loaded document with {len(doc.page_content)} characters\")\n",
    "                doc.metadata[\"source_file\"] = pdf_file.name\n",
    "                doc.metadata['file_type']='pdf'\n",
    "            \n",
    "            all_documents.extend(documents)\n",
    "            print(f\" pages loaded: {len(documents)}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {pdf_file.name}: {e}\")\n",
    "\n",
    "    return all_documents\n",
    "\n",
    "all_pdf_documents = process_all_pdfs(\"../data/pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9f9779e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split 9 into 16 chunks\n",
      "\n",
      "Example chunks\n",
      "content: Indian Institute of Technology Jodhpur\n",
      "Fundamentals of Distributed Systems\n",
      "Assignment – 2\n",
      "Total Marks:\n",
      "20\n",
      "Submission Deadline:\n",
      "27 July 2025\n",
      "metadata: {'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-22T09:44:08+00:00', 'source': '../data/pdf/Assigntment 2.pdf', 'file_path': '../data/pdf/Assigntment 2.pdf', 'total_pages': 3, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-07-22T09:44:08+00:00', 'trapped': '', 'modDate': 'D:20250722094408Z', 'creationDate': 'D:20250722094408Z', 'page': 0, 'source_file': 'Assigntment 2.pdf', 'file_type': 'pdf'}\n"
     ]
    }
   ],
   "source": [
    "# text splitting into chunks\n",
    "def split_documents(documents, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"\n",
    "    Split documents into smaller chunks for better RAG performance\n",
    "    \"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\",\"\\n\",\" \",\"\"]\n",
    "    )\n",
    "    split_docs = text_splitter.split_documents(documents)\n",
    "    print(f\"split {len(documents)} into {len(split_docs)} chunks\")\n",
    "\n",
    "    if split_docs:\n",
    "        print(\"\\nExample chunks\")\n",
    "        print(f\"content: {split_docs[0].page_content[:200]}\")\n",
    "        print(f\"metadata: {split_docs[0].metadata}\")\n",
    "\n",
    "    return split_docs\n",
    "\n",
    "chunks = split_documents(all_pdf_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749626d9",
   "metadata": {},
   "source": [
    "### Embeddings and vectorStoreDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e7e461b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from chromadb.config import Settings\n",
    "import uuid\n",
    "from typing import List, Dict, Tuple, Any\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b6cccb7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: all-MiniLM-L6-v2\n",
      "Model loaded successfully. Embedding dimension: 384\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.EmbeddingManager at 0x169ff3230>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class EmbeddingManager:\n",
    "    \"\"\"\n",
    "    Handles document embedding generation using SentenceTransformer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):\n",
    "        \"\"\"\n",
    "        Initialize the EmbeddingManager with a specified model.\n",
    "\n",
    "        :param model_name: Name of the SentenceTransformer model to use.\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self._load_model()\n",
    "\n",
    "    def _load_model(self):\n",
    "        \"\"\"\n",
    "        Load the SentenceTransformer model.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(f\"Loading model: {self.model_name}\")\n",
    "            self.model = SentenceTransformer(self.model_name)\n",
    "            print(f\"Model loaded successfully. Embedding dimension: {self.model.get_sentence_embedding_dimension()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model {self.model_name}: {e}\")\n",
    "            raise\n",
    "\n",
    "    def generate_embeddings(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Generate embeddings for a list of texts.\n",
    "\n",
    "        :param texts: List of strings to embed.\n",
    "        :return: Numpy array of embeddings.\n",
    "        \"\"\"\n",
    "        if not self.model:\n",
    "            raise ValueError(\"Model is not loaded.\")\n",
    "        \n",
    "        try:\n",
    "            embeddings = self.model.encode(texts, convert_to_numpy=True, show_progress_bar=True)\n",
    "            print(f\"Generated embeddings for {len(texts)} texts with shape {embeddings.shape}\")\n",
    "            return embeddings\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating embeddings: {e}\")\n",
    "            raise\n",
    "\n",
    "    def get_embedding_dimension(self) -> int:\n",
    "        \"\"\"\n",
    "        Get the dimension of the embeddings produced by the model.\n",
    "\n",
    "        :return: Embedding dimension as an integer.\n",
    "        \"\"\"\n",
    "        if not self.model:\n",
    "            raise ValueError(\"Model is not loaded.\")\n",
    "        \n",
    "        return self.model.get_sentence_embedding_dimension()\n",
    "    \n",
    "#  Initilizing the embedding manager\n",
    "embedding_manager = EmbeddingManager()\n",
    "embedding_manager"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff1a7bd",
   "metadata": {},
   "source": [
    "### VectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "48f1c215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store initialized with collection: pdf_documents\n",
      "Existing number of documents in store: 32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.VectorStore at 0x169ff30e0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class VectorStore:\n",
    "    \"\"\"\n",
    "    Manages document embeddings in a chromadb vector store.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, collection_name: str = \"pdf_documents\", persist_directory: str = \"../data/vector_store\"):\n",
    "        \"\"\"\n",
    "        Initialize the VectorStore with a specified collection name and persistence directory.\n",
    "\n",
    "        :param collection_name: Name of the chromadb collection.\n",
    "        :param persist_directory: Directory to persist the vector store data.\n",
    "        \"\"\"\n",
    "        self.collection_name = collection_name\n",
    "        self.persist_directory = persist_directory\n",
    "        self.client = None\n",
    "        self.collection = None\n",
    "        self._initialize_store()\n",
    "\n",
    "    def _initialize_store(self):\n",
    "        \"\"\"\n",
    "        Initialize the chromadb client and collection.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            os.makedirs(self.persist_directory, exist_ok=True)\n",
    "            self.client = chromadb.PersistentClient(path=self.persist_directory)\n",
    "\n",
    "            self.collection = self.client.get_or_create_collection(\n",
    "                name = self.collection_name,\n",
    "                metadata={\n",
    "                    \"description\": \"PDF documents embeddings for RAG\",\n",
    "                }\n",
    "            )\n",
    "\n",
    "            print(f\"Vector store initialized with collection: {self.collection_name}\")\n",
    "            print(f\"Existing number of documents in store: {self.collection.count()}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing vector store: {e}\")\n",
    "            raise\n",
    "\n",
    "    def add_documents(self, documents: List[Any], embeddings: np.ndarray):\n",
    "        \"\"\"\n",
    "        Add documents and their embeddings to the vector store.\n",
    "\n",
    "        :param documents: List of document objects with metadata.\n",
    "        :param embeddings: Numpy array of embeddings corresponding to the documents.\n",
    "        \"\"\"\n",
    "        if len(documents) != len(embeddings):\n",
    "            raise ValueError(\"Number of documents and embeddings must match.\")\n",
    "        \n",
    "        print(f\"Adding {len(documents)} documents to the vector store.\")\n",
    "\n",
    "        # prepare data for metadata\n",
    "        ids = []\n",
    "        metadatas = []\n",
    "        document_text = []\n",
    "        embedding_list = []\n",
    "\n",
    "        for i, (doc, embeddding) in enumerate(zip(documents, embeddings)):\n",
    "            doc_id = f\"doc_{uuid.uuid4().hex[:8]}_{i}\"\n",
    "            ids.append(doc_id)\n",
    "\n",
    "            metadata = dict(doc.metadata)  # ensure metadata is a dict\n",
    "            metadata['doc_index'] = i\n",
    "            metadata['content_length'] = len(doc.page_content)\n",
    "            metadatas.append(metadata)\n",
    "\n",
    "            document_text.append(doc.page_content)\n",
    "            embedding_list.append(embeddding.tolist())\n",
    "\n",
    "        # add to collection\n",
    "        try:\n",
    "            self.collection.add(\n",
    "                ids=ids,\n",
    "                embeddings=embedding_list,\n",
    "                metadatas=metadatas,\n",
    "                documents=document_text\n",
    "            )\n",
    "            print(f\"Successfully added {len(documents)} documents to the vector store.\")\n",
    "            print(f\"Total documents in store now: {self.collection.count()}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error adding documents to vector store: {e}\")\n",
    "            raise\n",
    "\n",
    "vectorstore = VectorStore()\n",
    "vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "11549ca8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-22T09:44:08+00:00', 'source': '../data/pdf/Assigntment 2.pdf', 'file_path': '../data/pdf/Assigntment 2.pdf', 'total_pages': 3, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-07-22T09:44:08+00:00', 'trapped': '', 'modDate': 'D:20250722094408Z', 'creationDate': 'D:20250722094408Z', 'page': 0, 'source_file': 'Assigntment 2.pdf', 'file_type': 'pdf'}, page_content='Indian Institute of Technology Jodhpur\\nFundamentals of Distributed Systems\\nAssignment – 2\\nTotal Marks:\\n20\\nSubmission Deadline:\\n27 July 2025'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-22T09:44:08+00:00', 'source': '../data/pdf/Assigntment 2.pdf', 'file_path': '../data/pdf/Assigntment 2.pdf', 'total_pages': 3, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-07-22T09:44:08+00:00', 'trapped': '', 'modDate': 'D:20250722094408Z', 'creationDate': 'D:20250722094408Z', 'page': 1, 'source_file': 'Assigntment 2.pdf', 'file_type': 'pdf'}, page_content='Datasets\\n• Cruise data: Cruise CSV file (click here to download)\\n• Customer churn data: Customer Churn CSV file (click here to download)\\n• E-commerce customer data: E-commerce Customer CSV file (click here to down-\\nload)\\nInstructions\\n• Implement all MapReduce jobs using the mrjob library and Hadoop in Google Colab.\\n• At the top of your notebook, install dependencies and setup hadoop.\\n• Load each CSV directly from the URLs above using wget or curl command into the\\nGoogle Colab.\\n• For each question:\\n1. Write mapper, reducer (and combiners or multi-step definitions) as mrjob classes.\\n2. Include a brief docstring explaining your design in Google Colab using markdown\\nfeature for each question and cell of Colab.\\n3. Demonstrate correctness on a small inline example.\\n• Name your notebook Assignment2-(Roll No of Yours)-(Name of yours).ipynb\\nand submit a link to GitHub or Colab and also submit the Jupyter notebook file in LMS.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-22T09:44:08+00:00', 'source': '../data/pdf/Assigntment 2.pdf', 'file_path': '../data/pdf/Assigntment 2.pdf', 'total_pages': 3, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-07-22T09:44:08+00:00', 'trapped': '', 'modDate': 'D:20250722094408Z', 'creationDate': 'D:20250722094408Z', 'page': 1, 'source_file': 'Assigntment 2.pdf', 'file_type': 'pdf'}, page_content='• Name your notebook Assignment2-(Roll No of Yours)-(Name of yours).ipynb\\nand submit a link to GitHub or Colab and also submit the Jupyter notebook file in LMS.\\n• At the end, include a cell that runs all jobs on full datasets and shows final outputs.\\nQuestions\\n1. Cruiseline Aggregations (5 marks)\\nUsing cruise.csv, implement an mrjob class that computes, for each Cruise line:\\n(a) Total number of ships.\\n(b) Average Tonnage (to two decimals).\\n(c) Maximum crew size.\\n(Optional) Use a Combiner for partial aggregation.\\n2. Company Churn Rate (5 marks)\\nFrom customer churn.csv, create a MultiStepJob:\\nStep 1: Mapper emits (Company, TOTAL) and (Company, CHURNED) where Churn==1.\\nStep 2: Reducer computes churn rate = CHURNED\\nTOTAL , outputting four-decimal floats.\\nUse a small VIP companies.txt in the distributed cache to restrict to listed companies.\\nProvide a sample file with at least three names.\\n1'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-22T09:44:08+00:00', 'source': '../data/pdf/Assigntment 2.pdf', 'file_path': '../data/pdf/Assigntment 2.pdf', 'total_pages': 3, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-07-22T09:44:08+00:00', 'trapped': '', 'modDate': 'D:20250722094408Z', 'creationDate': 'D:20250722094408Z', 'page': 2, 'source_file': 'Assigntment 2.pdf', 'file_type': 'pdf'}, page_content='3. State-wise Spending (5 marks)\\nFrom e-com customer.csv, extract the two-letter state code from the Address field.\\nThen:\\n• Mapper parses the state.\\n• Reducer sums Yearly Amount Spent per state.\\n• Output the top 5 states by total spending.\\n4. Two-step Ship Filter & Median Length (5 marks)\\nOn cruise.csv, implement a two-step mrjob pipeline:\\nStep 1: Filter ships with passenger density > 35.0; emit ⟨Cruise line, length⟩.\\nStep 2: Compute the median of the lengths per Cruise line, handling even/odd counts\\ncorrectly.\\nUse the steps() API and output medians to two decimals.\\nSubmission\\n• Submit Assignment2.ipynb with all code, inline tests, proper brief markup comments\\nfor each question and final outputs.\\n• Ensure each question’s results are clearly labeled.\\n• Provide a GitHub or Colab link for assessment.\\n2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-13T15:44:27+00:00', 'source': '../data/pdf/Assignment 1.pdf', 'file_path': '../data/pdf/Assignment 1.pdf', 'total_pages': 5, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-13T15:44:27+00:00', 'trapped': '', 'modDate': 'D:20250613154427Z', 'creationDate': 'D:20250613154427Z', 'page': 0, 'source_file': 'Assignment 1.pdf', 'file_type': 'pdf'}, page_content='Indian Institute of Technology Jodhpur\\nFundamentals of Distributed Systems\\nAssignment – 1\\nTotal Marks:\\n20\\nSubmission Deadline:\\n23 June 2025'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-13T15:44:27+00:00', 'source': '../data/pdf/Assignment 1.pdf', 'file_path': '../data/pdf/Assignment 1.pdf', 'total_pages': 5, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-13T15:44:27+00:00', 'trapped': '', 'modDate': 'D:20250613154427Z', 'creationDate': 'D:20250613154427Z', 'page': 1, 'source_file': 'Assignment 1.pdf', 'file_type': 'pdf'}, page_content='1. Vector Clocks and Causal Ordering\\n[10 Marks]\\nObjective\\nTo move beyond simple event ordering by implementing Vector Clocks to capture the causal\\nrelationships between events in a distributed system. You will apply this to build a causally\\nconsistent, multi-node key-value store.\\nProblem Description\\nYou will build a distributed key-value store with three or more nodes. The key challenge is\\nto ensure that writes to the store are causally ordered. If event B is causally dependent on\\nevent A (e.g., a value is read and then updated), all nodes must process event A before they\\nprocess event B. Simple Lamport clocks are insufficient for this, so you will use Vector Clocks.\\nTechnology Constraints\\n• Programming Language: The entire application logic for the nodes and client must\\nbe written exclusively in Python.\\n• Containerization: The system must be containerized and orchestrated solely using\\nDocker and Docker Compose.\\nTasks\\nYour implementation should cover the following tasks:'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-13T15:44:27+00:00', 'source': '../data/pdf/Assignment 1.pdf', 'file_path': '../data/pdf/Assignment 1.pdf', 'total_pages': 5, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-13T15:44:27+00:00', 'trapped': '', 'modDate': 'D:20250613154427Z', 'creationDate': 'D:20250613154427Z', 'page': 1, 'source_file': 'Assignment 1.pdf', 'file_type': 'pdf'}, page_content='• Containerization: The system must be containerized and orchestrated solely using\\nDocker and Docker Compose.\\nTasks\\nYour implementation should cover the following tasks:\\n1. Node Implementation with Vector Clocks: Create a Python script for a node. Each\\nnode must maintain its own local key-value data and a Vector Clock.\\n2. Vector Clock Logic: Implement the rules for incrementing the clock on local events,\\nincluding the clock in sent messages, and updating the local clock upon receiving a\\nmessage.\\n3. Causal Write Propagation: Implement the Causal Delivery Rule.\\nWhen a node\\nreceives a replicated write message, it must delay processing that write until the causal\\ndependencies are met by checking the message’s vector clock against its own. Messages\\nthat cannot be delivered must be buffered.\\n4. Containerization and Networking: Write a ‘Dockerfile‘ for your node and a ‘docker-\\ncompose.yml‘ file to run a 3-node system.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-13T15:44:27+00:00', 'source': '../data/pdf/Assignment 1.pdf', 'file_path': '../data/pdf/Assignment 1.pdf', 'total_pages': 5, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-13T15:44:27+00:00', 'trapped': '', 'modDate': 'D:20250613154427Z', 'creationDate': 'D:20250613154427Z', 'page': 1, 'source_file': 'Assignment 1.pdf', 'file_type': 'pdf'}, page_content='that cannot be delivered must be buffered.\\n4. Containerization and Networking: Write a ‘Dockerfile‘ for your node and a ‘docker-\\ncompose.yml‘ file to run a 3-node system.\\n5. Verification and Scenario Testing: Create a client script and a specific test scenario\\nto prove that your system maintains causal consistency, even when messages arrive out\\nof order.\\n1'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-13T15:44:27+00:00', 'source': '../data/pdf/Assignment 1.pdf', 'file_path': '../data/pdf/Assignment 1.pdf', 'total_pages': 5, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-13T15:44:27+00:00', 'trapped': '', 'modDate': 'D:20250613154427Z', 'creationDate': 'D:20250613154427Z', 'page': 2, 'source_file': 'Assignment 1.pdf', 'file_type': 'pdf'}, page_content='Deliverables\\nYour final submission must be a single Git repository containing all the required files organized\\nin the exact structure shown below.\\nFolder and File Structure\\n1 vector -clock -kv -store/\\n2 |\\n3 |-- src/\\n4 |\\n|-- node.py\\n5 |\\n‘-- client.py\\n6 |\\n7 |-- Dockerfile\\n8 |-- docker -compose.yml\\n9 ‘-- project_report .pdf\\nProject Report Requirements\\nYou must submit a thorough Project Report that details your work.\\n• Format: The report must be a typed document (e.g., created in Microsoft Word,\\nGoogle Docs, or LATEX) and submitted as a single ‘project report.pdf‘ file. Handwritten\\nreports will not be accepted.\\n• Content: The report should detail your architecture and implementation and use logs\\nwith screenshots to prove that your causal consistency test works correctly.\\n• Video Link: The report must include a publicly accessible link to a short (max 3-\\nminute) video demonstrating the project.\\n2. Dynamic Load Balancing for a Smart Grid\\n[10 Marks]\\nObjective'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-13T15:44:27+00:00', 'source': '../data/pdf/Assignment 1.pdf', 'file_path': '../data/pdf/Assignment 1.pdf', 'total_pages': 5, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-13T15:44:27+00:00', 'trapped': '', 'modDate': 'D:20250613154427Z', 'creationDate': 'D:20250613154427Z', 'page': 2, 'source_file': 'Assignment 1.pdf', 'file_type': 'pdf'}, page_content='• Video Link: The report must include a publicly accessible link to a short (max 3-\\nminute) video demonstrating the project.\\n2. Dynamic Load Balancing for a Smart Grid\\n[10 Marks]\\nObjective\\nTo design and build a scalable system for a Smart Grid that dynamically balances Electric\\nVehicle (EV) charging requests across multiple substations based on their real-time load,\\ncomplete with a comprehensive observability stack.\\nProblem Description\\nYou will build a system that simulates a Smart Grid managing charging requests from a\\nfleet of EVs. The primary challenge is to prevent overloading any single charging substation.\\nThe system must intelligently distribute incoming charging requests to the least loaded\\nsubstation, ensuring grid stability and efficient resource usage.\\nYou will use an industry-\\nstandard monitoring stack to measure and visualize key performance indicators.\\n2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-13T15:44:27+00:00', 'source': '../data/pdf/Assignment 1.pdf', 'file_path': '../data/pdf/Assignment 1.pdf', 'total_pages': 5, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-13T15:44:27+00:00', 'trapped': '', 'modDate': 'D:20250613154427Z', 'creationDate': 'D:20250613154427Z', 'page': 3, 'source_file': 'Assignment 1.pdf', 'file_type': 'pdf'}, page_content='Technology Constraints\\n• Programming Language: All custom services must be written exclusively in Python.\\n• Containerization & Orchestration: The entire system must be defined, configured,\\nand run using Docker and Docker Compose.\\nTasks\\nYour implementation should cover the following tasks:\\n1. Microservice Development: Create two services: a ‘charge request service‘ as the\\npublic entry point and a ‘substation service‘ that simulates charging. Instrument the\\nsubstation to expose its current load as a Prometheus metric.\\n2. Custom Dynamic Load Balancer: Build a new service that acts as the grid’s core\\nlogic. It must periodically poll the ‘/metrics‘ endpoint of each substation to get its\\ncurrent load and use this data to decide where to route new requests.\\n3. Observability Stack: Configure Prometheus to scrape the substation metrics and\\nGrafana to visualize the load on a dashboard.\\n4. Containerization & Orchestration: Write ‘Dockerfile‘s for all services and a ‘docker-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-13T15:44:27+00:00', 'source': '../data/pdf/Assignment 1.pdf', 'file_path': '../data/pdf/Assignment 1.pdf', 'total_pages': 5, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-13T15:44:27+00:00', 'trapped': '', 'modDate': 'D:20250613154427Z', 'creationDate': 'D:20250613154427Z', 'page': 3, 'source_file': 'Assignment 1.pdf', 'file_type': 'pdf'}, page_content='Grafana to visualize the load on a dashboard.\\n4. Containerization & Orchestration: Write ‘Dockerfile‘s for all services and a ‘docker-\\ncompose.yml‘ file to run the entire system, including multiple replicas of the substation\\nservice.\\n5. Load Testing and Analysis: Create a Python script to simulate a ”rush hour” of EV\\ncharging requests and analyze the system’s response on your Grafana dashboard.\\nDeliverables\\nYour final submission must be a single Git repository containing all the required files organized\\nin the exact structure shown below.\\nFolder and File Structure\\n1 smart -grid -load -balancer/\\n2 |\\n3 |-- charge_request_service /\\n4 |\\n|-- main.py\\n5 |\\n‘-- Dockerfile\\n6 |\\n7 |-- load_balancer/\\n8 |\\n|-- main.py\\n9 |\\n‘-- Dockerfile\\n10 |\\n11 |-- substation_service /\\n12 |\\n|-- main.py\\n13 |\\n‘-- Dockerfile\\n14 |\\n15 |-- load_tester/\\n16 |\\n‘-- test.py\\n17 |\\n18 |-- monitoring/\\n3'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-06-13T15:44:27+00:00', 'source': '../data/pdf/Assignment 1.pdf', 'file_path': '../data/pdf/Assignment 1.pdf', 'total_pages': 5, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-06-13T15:44:27+00:00', 'trapped': '', 'modDate': 'D:20250613154427Z', 'creationDate': 'D:20250613154427Z', 'page': 4, 'source_file': 'Assignment 1.pdf', 'file_type': 'pdf'}, page_content='19 |\\n|-- prometheus/\\n20 |\\n|\\n‘-- prometheus.yml\\n21 |\\n‘-- grafana/\\n22 |\\n‘-- dashboard.json\\n23 |\\n24 |-- docker -compose.yml\\n25 ‘-- project_report .pdf\\nProject Report Requirements\\nYou must submit a thorough Project Report that details your work.\\n• Format: The report must be a typed document and submitted as a single project report.pdf\\nfile. Handwritten reports will not be accepted.\\n• Content: The report should detail your architecture and analyze the system’s perfor-\\nmance using screenshots from your Grafana dashboard during the load test.\\n• Video Link: The report must include a publicly accessible link to a short (max 3-\\nminute) video demonstrating the project in action.\\n4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24', 'creator': 'TeX', 'creationdate': '2025-04-12T18:02:53+05:30', 'source': '../data/pdf/assignment.pdf', 'file_path': '../data/pdf/assignment.pdf', 'total_pages': 1, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-04-12T18:02:53+05:30', 'trapped': '', 'modDate': \"D:20250412180253+05'30'\", 'creationDate': \"D:20250412180253+05'30'\", 'page': 0, 'source_file': 'assignment.pdf', 'file_type': 'pdf'}, page_content='Assignment\\n1. A drawer contains two coins. One is an unbiased coin, which when tossed, is equally likely to turn up heads or\\ntails. The other is a biased coin, which will turn up heads with probability p and tails with probability 1 −p.\\nOne coin is selected (uniformly) at random from the drawer. Two experiments are performed:\\n(i) The selected coin is tossed n times. Given that the coin turns up heads k times and tails n−k times, what\\nis the probability that the coin is biased?\\n(ii) The selected coin is toss repeatedly until it turns up heads k times. Given that the coin is tossed n times in\\ntotal, what is the probability that the coin is biased?\\n2. Which of the following functions are probability density functions:\\n(i) f(x) = x(2−x), 0 < x < 2, and 0 elsewhere.\\n(ii) f(x) = 1\\nλ e−(x−θ)/λ, x > θ, and 0 elsewhere, λ > 0.\\n(iii) f(x) = sinx, 0 < x < π/2, and 0 elsewhere.\\n3. A workstation consists of three machines, M1, M2 and M3, each of which will fail after an amount of time Ti'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24', 'creator': 'TeX', 'creationdate': '2025-04-12T18:02:53+05:30', 'source': '../data/pdf/assignment.pdf', 'file_path': '../data/pdf/assignment.pdf', 'total_pages': 1, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-04-12T18:02:53+05:30', 'trapped': '', 'modDate': \"D:20250412180253+05'30'\", 'creationDate': \"D:20250412180253+05'30'\", 'page': 0, 'source_file': 'assignment.pdf', 'file_type': 'pdf'}, page_content='(iii) f(x) = sinx, 0 < x < π/2, and 0 elsewhere.\\n3. A workstation consists of three machines, M1, M2 and M3, each of which will fail after an amount of time Ti\\nwhich is an independent exponentially distributed random variable, with parameter 1. Assume that the times to\\nfailure of the different machines are independent. The workstation fails as soon as both of the following have\\nhappened:\\n(i) Machine M1 has failed.\\n(ii) Atleast one of the machines M2 or M3 has failed.\\nFind the expected value of the time to failure of the workstation.\\n4. A and B throw alternatively with a pair of balanced dice. A wins if he throws a sum of 6 points before B throws\\na sum of 7 points, while B wins if he throws a sum of 7 points before A throws a sum of 6 points. Suppose that\\nA begins the game. What is the probability that A will win the game?\\n5. A post office has 2 clerks. Nikita enters the post office while 2 other customers, Rohan and Ankita, are being'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24', 'creator': 'TeX', 'creationdate': '2025-04-12T18:02:53+05:30', 'source': '../data/pdf/assignment.pdf', 'file_path': '../data/pdf/assignment.pdf', 'total_pages': 1, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-04-12T18:02:53+05:30', 'trapped': '', 'modDate': \"D:20250412180253+05'30'\", 'creationDate': \"D:20250412180253+05'30'\", 'page': 0, 'source_file': 'assignment.pdf', 'file_type': 'pdf'}, page_content='A begins the game. What is the probability that A will win the game?\\n5. A post office has 2 clerks. Nikita enters the post office while 2 other customers, Rohan and Ankita, are being\\nserved by the 2 clerks. She is next in line. Assume that the time a clerk spends serving a customer has the\\nExponential(λ) distribution.\\n(a) What is the probability that Nikita is the last of the 3 customers to be done being served?\\n(b) What is the expected total time that Nikita needs to spend at the post office?')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cd4e08be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Indian Institute of Technology Jodhpur\\nFundamentals of Distributed Systems\\nAssignment – 2\\nTotal Marks:\\n20\\nSubmission Deadline:\\n27 July 2025',\n",
       " 'Datasets\\n• Cruise data: Cruise CSV file (click here to download)\\n• Customer churn data: Customer Churn CSV file (click here to download)\\n• E-commerce customer data: E-commerce Customer CSV file (click here to down-\\nload)\\nInstructions\\n• Implement all MapReduce jobs using the mrjob library and Hadoop in Google Colab.\\n• At the top of your notebook, install dependencies and setup hadoop.\\n• Load each CSV directly from the URLs above using wget or curl command into the\\nGoogle Colab.\\n• For each question:\\n1. Write mapper, reducer (and combiners or multi-step definitions) as mrjob classes.\\n2. Include a brief docstring explaining your design in Google Colab using markdown\\nfeature for each question and cell of Colab.\\n3. Demonstrate correctness on a small inline example.\\n• Name your notebook Assignment2-(Roll No of Yours)-(Name of yours).ipynb\\nand submit a link to GitHub or Colab and also submit the Jupyter notebook file in LMS.',\n",
       " '• Name your notebook Assignment2-(Roll No of Yours)-(Name of yours).ipynb\\nand submit a link to GitHub or Colab and also submit the Jupyter notebook file in LMS.\\n• At the end, include a cell that runs all jobs on full datasets and shows final outputs.\\nQuestions\\n1. Cruiseline Aggregations (5 marks)\\nUsing cruise.csv, implement an mrjob class that computes, for each Cruise line:\\n(a) Total number of ships.\\n(b) Average Tonnage (to two decimals).\\n(c) Maximum crew size.\\n(Optional) Use a Combiner for partial aggregation.\\n2. Company Churn Rate (5 marks)\\nFrom customer churn.csv, create a MultiStepJob:\\nStep 1: Mapper emits (Company, TOTAL) and (Company, CHURNED) where Churn==1.\\nStep 2: Reducer computes churn rate = CHURNED\\nTOTAL , outputting four-decimal floats.\\nUse a small VIP companies.txt in the distributed cache to restrict to listed companies.\\nProvide a sample file with at least three names.\\n1',\n",
       " '3. State-wise Spending (5 marks)\\nFrom e-com customer.csv, extract the two-letter state code from the Address field.\\nThen:\\n• Mapper parses the state.\\n• Reducer sums Yearly Amount Spent per state.\\n• Output the top 5 states by total spending.\\n4. Two-step Ship Filter & Median Length (5 marks)\\nOn cruise.csv, implement a two-step mrjob pipeline:\\nStep 1: Filter ships with passenger density > 35.0; emit ⟨Cruise line, length⟩.\\nStep 2: Compute the median of the lengths per Cruise line, handling even/odd counts\\ncorrectly.\\nUse the steps() API and output medians to two decimals.\\nSubmission\\n• Submit Assignment2.ipynb with all code, inline tests, proper brief markup comments\\nfor each question and final outputs.\\n• Ensure each question’s results are clearly labeled.\\n• Provide a GitHub or Colab link for assessment.\\n2',\n",
       " 'Indian Institute of Technology Jodhpur\\nFundamentals of Distributed Systems\\nAssignment – 1\\nTotal Marks:\\n20\\nSubmission Deadline:\\n23 June 2025',\n",
       " '1. Vector Clocks and Causal Ordering\\n[10 Marks]\\nObjective\\nTo move beyond simple event ordering by implementing Vector Clocks to capture the causal\\nrelationships between events in a distributed system. You will apply this to build a causally\\nconsistent, multi-node key-value store.\\nProblem Description\\nYou will build a distributed key-value store with three or more nodes. The key challenge is\\nto ensure that writes to the store are causally ordered. If event B is causally dependent on\\nevent A (e.g., a value is read and then updated), all nodes must process event A before they\\nprocess event B. Simple Lamport clocks are insufficient for this, so you will use Vector Clocks.\\nTechnology Constraints\\n• Programming Language: The entire application logic for the nodes and client must\\nbe written exclusively in Python.\\n• Containerization: The system must be containerized and orchestrated solely using\\nDocker and Docker Compose.\\nTasks\\nYour implementation should cover the following tasks:',\n",
       " '• Containerization: The system must be containerized and orchestrated solely using\\nDocker and Docker Compose.\\nTasks\\nYour implementation should cover the following tasks:\\n1. Node Implementation with Vector Clocks: Create a Python script for a node. Each\\nnode must maintain its own local key-value data and a Vector Clock.\\n2. Vector Clock Logic: Implement the rules for incrementing the clock on local events,\\nincluding the clock in sent messages, and updating the local clock upon receiving a\\nmessage.\\n3. Causal Write Propagation: Implement the Causal Delivery Rule.\\nWhen a node\\nreceives a replicated write message, it must delay processing that write until the causal\\ndependencies are met by checking the message’s vector clock against its own. Messages\\nthat cannot be delivered must be buffered.\\n4. Containerization and Networking: Write a ‘Dockerfile‘ for your node and a ‘docker-\\ncompose.yml‘ file to run a 3-node system.',\n",
       " 'that cannot be delivered must be buffered.\\n4. Containerization and Networking: Write a ‘Dockerfile‘ for your node and a ‘docker-\\ncompose.yml‘ file to run a 3-node system.\\n5. Verification and Scenario Testing: Create a client script and a specific test scenario\\nto prove that your system maintains causal consistency, even when messages arrive out\\nof order.\\n1',\n",
       " 'Deliverables\\nYour final submission must be a single Git repository containing all the required files organized\\nin the exact structure shown below.\\nFolder and File Structure\\n1 vector -clock -kv -store/\\n2 |\\n3 |-- src/\\n4 |\\n|-- node.py\\n5 |\\n‘-- client.py\\n6 |\\n7 |-- Dockerfile\\n8 |-- docker -compose.yml\\n9 ‘-- project_report .pdf\\nProject Report Requirements\\nYou must submit a thorough Project Report that details your work.\\n• Format: The report must be a typed document (e.g., created in Microsoft Word,\\nGoogle Docs, or LATEX) and submitted as a single ‘project report.pdf‘ file. Handwritten\\nreports will not be accepted.\\n• Content: The report should detail your architecture and implementation and use logs\\nwith screenshots to prove that your causal consistency test works correctly.\\n• Video Link: The report must include a publicly accessible link to a short (max 3-\\nminute) video demonstrating the project.\\n2. Dynamic Load Balancing for a Smart Grid\\n[10 Marks]\\nObjective',\n",
       " '• Video Link: The report must include a publicly accessible link to a short (max 3-\\nminute) video demonstrating the project.\\n2. Dynamic Load Balancing for a Smart Grid\\n[10 Marks]\\nObjective\\nTo design and build a scalable system for a Smart Grid that dynamically balances Electric\\nVehicle (EV) charging requests across multiple substations based on their real-time load,\\ncomplete with a comprehensive observability stack.\\nProblem Description\\nYou will build a system that simulates a Smart Grid managing charging requests from a\\nfleet of EVs. The primary challenge is to prevent overloading any single charging substation.\\nThe system must intelligently distribute incoming charging requests to the least loaded\\nsubstation, ensuring grid stability and efficient resource usage.\\nYou will use an industry-\\nstandard monitoring stack to measure and visualize key performance indicators.\\n2',\n",
       " 'Technology Constraints\\n• Programming Language: All custom services must be written exclusively in Python.\\n• Containerization & Orchestration: The entire system must be defined, configured,\\nand run using Docker and Docker Compose.\\nTasks\\nYour implementation should cover the following tasks:\\n1. Microservice Development: Create two services: a ‘charge request service‘ as the\\npublic entry point and a ‘substation service‘ that simulates charging. Instrument the\\nsubstation to expose its current load as a Prometheus metric.\\n2. Custom Dynamic Load Balancer: Build a new service that acts as the grid’s core\\nlogic. It must periodically poll the ‘/metrics‘ endpoint of each substation to get its\\ncurrent load and use this data to decide where to route new requests.\\n3. Observability Stack: Configure Prometheus to scrape the substation metrics and\\nGrafana to visualize the load on a dashboard.\\n4. Containerization & Orchestration: Write ‘Dockerfile‘s for all services and a ‘docker-',\n",
       " 'Grafana to visualize the load on a dashboard.\\n4. Containerization & Orchestration: Write ‘Dockerfile‘s for all services and a ‘docker-\\ncompose.yml‘ file to run the entire system, including multiple replicas of the substation\\nservice.\\n5. Load Testing and Analysis: Create a Python script to simulate a ”rush hour” of EV\\ncharging requests and analyze the system’s response on your Grafana dashboard.\\nDeliverables\\nYour final submission must be a single Git repository containing all the required files organized\\nin the exact structure shown below.\\nFolder and File Structure\\n1 smart -grid -load -balancer/\\n2 |\\n3 |-- charge_request_service /\\n4 |\\n|-- main.py\\n5 |\\n‘-- Dockerfile\\n6 |\\n7 |-- load_balancer/\\n8 |\\n|-- main.py\\n9 |\\n‘-- Dockerfile\\n10 |\\n11 |-- substation_service /\\n12 |\\n|-- main.py\\n13 |\\n‘-- Dockerfile\\n14 |\\n15 |-- load_tester/\\n16 |\\n‘-- test.py\\n17 |\\n18 |-- monitoring/\\n3',\n",
       " '19 |\\n|-- prometheus/\\n20 |\\n|\\n‘-- prometheus.yml\\n21 |\\n‘-- grafana/\\n22 |\\n‘-- dashboard.json\\n23 |\\n24 |-- docker -compose.yml\\n25 ‘-- project_report .pdf\\nProject Report Requirements\\nYou must submit a thorough Project Report that details your work.\\n• Format: The report must be a typed document and submitted as a single project report.pdf\\nfile. Handwritten reports will not be accepted.\\n• Content: The report should detail your architecture and analyze the system’s perfor-\\nmance using screenshots from your Grafana dashboard during the load test.\\n• Video Link: The report must include a publicly accessible link to a short (max 3-\\nminute) video demonstrating the project in action.\\n4',\n",
       " 'Assignment\\n1. A drawer contains two coins. One is an unbiased coin, which when tossed, is equally likely to turn up heads or\\ntails. The other is a biased coin, which will turn up heads with probability p and tails with probability 1 −p.\\nOne coin is selected (uniformly) at random from the drawer. Two experiments are performed:\\n(i) The selected coin is tossed n times. Given that the coin turns up heads k times and tails n−k times, what\\nis the probability that the coin is biased?\\n(ii) The selected coin is toss repeatedly until it turns up heads k times. Given that the coin is tossed n times in\\ntotal, what is the probability that the coin is biased?\\n2. Which of the following functions are probability density functions:\\n(i) f(x) = x(2−x), 0 < x < 2, and 0 elsewhere.\\n(ii) f(x) = 1\\nλ e−(x−θ)/λ, x > θ, and 0 elsewhere, λ > 0.\\n(iii) f(x) = sinx, 0 < x < π/2, and 0 elsewhere.\\n3. A workstation consists of three machines, M1, M2 and M3, each of which will fail after an amount of time Ti',\n",
       " '(iii) f(x) = sinx, 0 < x < π/2, and 0 elsewhere.\\n3. A workstation consists of three machines, M1, M2 and M3, each of which will fail after an amount of time Ti\\nwhich is an independent exponentially distributed random variable, with parameter 1. Assume that the times to\\nfailure of the different machines are independent. The workstation fails as soon as both of the following have\\nhappened:\\n(i) Machine M1 has failed.\\n(ii) Atleast one of the machines M2 or M3 has failed.\\nFind the expected value of the time to failure of the workstation.\\n4. A and B throw alternatively with a pair of balanced dice. A wins if he throws a sum of 6 points before B throws\\na sum of 7 points, while B wins if he throws a sum of 7 points before A throws a sum of 6 points. Suppose that\\nA begins the game. What is the probability that A will win the game?\\n5. A post office has 2 clerks. Nikita enters the post office while 2 other customers, Rohan and Ankita, are being',\n",
       " 'A begins the game. What is the probability that A will win the game?\\n5. A post office has 2 clerks. Nikita enters the post office while 2 other customers, Rohan and Ankita, are being\\nserved by the 2 clerks. She is next in line. Assume that the time a clerk spends serving a customer has the\\nExponential(λ) distribution.\\n(a) What is the probability that Nikita is the last of the 3 customers to be done being served?\\n(b) What is the expected total time that Nikita needs to spend at the post office?']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert the text to embeddings\n",
    "texts = [doc.page_content for doc in chunks]\n",
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9ae05f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings for 16 texts with shape (16, 384)\n",
      "Adding 16 documents to the vector store.\n",
      "Successfully added 16 documents to the vector store.\n",
      "Total documents in store now: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# generate the embeddings\n",
    "embeddings = embedding_manager.generate_embeddings(texts)\n",
    "\n",
    "# store in vector database\n",
    "vectorstore.add_documents(chunks, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0511c988",
   "metadata": {},
   "source": [
    "### Retriever Pipeline from vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dd980cb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.RagRetriever at 0x169ff2120>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class RagRetriever:\n",
    "    \"\"\"\n",
    "    Retrieves relevant documents from the vector store based on query similarity.\n",
    "    \"\"\"\n",
    "    def __init__(self, vector_store: VectorStore, embedding_manager: EmbeddingManager):\n",
    "        \"\"\"\n",
    "        Initialize the Retriever with a vector store and embedding manager.\n",
    "        :param vector_store: Instance of VectorStore to retrieve documents from.\n",
    "        :param embedding_manager: Instance of EmbeddingManager to manage embeddings.\n",
    "        \"\"\"\n",
    "        self.vector_store = vector_store\n",
    "        self.embedding_manager = embedding_manager\n",
    "        \n",
    "    def retrieve(self, query: str, top_k: int = 5, score_threshold: float = 0.0) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Retrieve relevant documents based on the query.\n",
    "        :param query: The input query string.\n",
    "        :param top_k: Number of top relevant documents to retrieve.\n",
    "        :param score_threshold: Minimum score threshold for filtering documents.\n",
    "        :return: List of relevant documents with metadata.\n",
    "        \"\"\"\n",
    "        print(f\"Retrieving documents for query: {query}\")\n",
    "        print(f\"top K: {top_k}, score threshold: {score_threshold}\")\n",
    "\n",
    "        # generate query embeddings\n",
    "        query_embedding = self.embedding_manager.generate_embeddings([query])[0]\n",
    "\n",
    "        # search in vector store\n",
    "        try:\n",
    "            results = self.vector_store.collection.query(\n",
    "                query_embeddings=[query_embedding.tolist()],\n",
    "                n_results=top_k,\n",
    "            )\n",
    "\n",
    "            # process results\n",
    "            retrieved_docs = []\n",
    "\n",
    "            if results['documents'] and len(results['documents']) > 0:\n",
    "                documents = results['documents'][0]\n",
    "                metadatas = results['metadatas'][0]\n",
    "                distances = results['distances'][0]\n",
    "                ids = results['ids'][0]\n",
    "\n",
    "                for i, (doc, meta, dist, doc_id) in enumerate(zip(documents, metadatas, distances, ids)):\n",
    "                    similarity = 1 - dist  # convert distance to similarity\n",
    "                    if similarity >= score_threshold:\n",
    "                        retrieved_docs.append(\n",
    "                            {\n",
    "                                'id': doc_id,\n",
    "                                'content': doc,\n",
    "                                'metadata': meta,\n",
    "                                'similarity_score': similarity,\n",
    "                                'rank': i + 1\n",
    "                            }\n",
    "                        )\n",
    "                print(f\"Retrieved {len(retrieved_docs)} documents after applying score threshold.\")\n",
    "            else:\n",
    "                print(\"No documents retrieved from the vector store.\")\n",
    "\n",
    "            return retrieved_docs\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error retrieving documents: {e}\")\n",
    "            raise\n",
    "\n",
    "rag_retriever = RagRetriever(vectorstore, embedding_manager)\n",
    "rag_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "33fd5aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: What do I have to do in Vector Clocks and Causal Ordering assignment?\n",
      "top K: 5, score threshold: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 83.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings for 1 texts with shape (1, 384)\n",
      "Retrieved 3 documents after applying score threshold.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'id': 'doc_4b5456c3_5',\n",
       "  'content': '1. Vector Clocks and Causal Ordering\\n[10 Marks]\\nObjective\\nTo move beyond simple event ordering by implementing Vector Clocks to capture the causal\\nrelationships between events in a distributed system. You will apply this to build a causally\\nconsistent, multi-node key-value store.\\nProblem Description\\nYou will build a distributed key-value store with three or more nodes. The key challenge is\\nto ensure that writes to the store are causally ordered. If event B is causally dependent on\\nevent A (e.g., a value is read and then updated), all nodes must process event A before they\\nprocess event B. Simple Lamport clocks are insufficient for this, so you will use Vector Clocks.\\nTechnology Constraints\\n• Programming Language: The entire application logic for the nodes and client must\\nbe written exclusively in Python.\\n• Containerization: The system must be containerized and orchestrated solely using\\nDocker and Docker Compose.\\nTasks\\nYour implementation should cover the following tasks:',\n",
       "  'metadata': {'total_pages': 5,\n",
       "   'content_length': 985,\n",
       "   'format': 'PDF 1.5',\n",
       "   'author': '',\n",
       "   'modDate': 'D:20250613154427Z',\n",
       "   'title': '',\n",
       "   'creationDate': 'D:20250613154427Z',\n",
       "   'creationdate': '2025-06-13T15:44:27+00:00',\n",
       "   'file_path': '../data/pdf/Assignment 1.pdf',\n",
       "   'creator': 'LaTeX with hyperref',\n",
       "   'source': '../data/pdf/Assignment 1.pdf',\n",
       "   'file_type': 'pdf',\n",
       "   'keywords': '',\n",
       "   'page': 1,\n",
       "   'source_file': 'Assignment 1.pdf',\n",
       "   'subject': '',\n",
       "   'producer': 'pdfTeX-1.40.26',\n",
       "   'moddate': '2025-06-13T15:44:27+00:00',\n",
       "   'trapped': '',\n",
       "   'doc_index': 5},\n",
       "  'similarity_score': 0.014769375324249268,\n",
       "  'rank': 1},\n",
       " {'id': 'doc_c82b0abb_5',\n",
       "  'content': '1. Vector Clocks and Causal Ordering\\n[10 Marks]\\nObjective\\nTo move beyond simple event ordering by implementing Vector Clocks to capture the causal\\nrelationships between events in a distributed system. You will apply this to build a causally\\nconsistent, multi-node key-value store.\\nProblem Description\\nYou will build a distributed key-value store with three or more nodes. The key challenge is\\nto ensure that writes to the store are causally ordered. If event B is causally dependent on\\nevent A (e.g., a value is read and then updated), all nodes must process event A before they\\nprocess event B. Simple Lamport clocks are insufficient for this, so you will use Vector Clocks.\\nTechnology Constraints\\n• Programming Language: The entire application logic for the nodes and client must\\nbe written exclusively in Python.\\n• Containerization: The system must be containerized and orchestrated solely using\\nDocker and Docker Compose.\\nTasks\\nYour implementation should cover the following tasks:',\n",
       "  'metadata': {'creationDate': 'D:20250613154427Z',\n",
       "   'moddate': '2025-06-13T15:44:27+00:00',\n",
       "   'file_path': '../data/pdf/Assignment 1.pdf',\n",
       "   'modDate': 'D:20250613154427Z',\n",
       "   'author': '',\n",
       "   'trapped': '',\n",
       "   'keywords': '',\n",
       "   'title': '',\n",
       "   'content_length': 985,\n",
       "   'doc_index': 5,\n",
       "   'file_type': 'pdf',\n",
       "   'total_pages': 5,\n",
       "   'format': 'PDF 1.5',\n",
       "   'producer': 'pdfTeX-1.40.26',\n",
       "   'source_file': 'Assignment 1.pdf',\n",
       "   'page': 1,\n",
       "   'subject': '',\n",
       "   'creator': 'LaTeX with hyperref',\n",
       "   'source': '../data/pdf/Assignment 1.pdf',\n",
       "   'creationdate': '2025-06-13T15:44:27+00:00'},\n",
       "  'similarity_score': 0.014769375324249268,\n",
       "  'rank': 2},\n",
       " {'id': 'doc_706c5cce_5',\n",
       "  'content': '1. Vector Clocks and Causal Ordering\\n[10 Marks]\\nObjective\\nTo move beyond simple event ordering by implementing Vector Clocks to capture the causal\\nrelationships between events in a distributed system. You will apply this to build a causally\\nconsistent, multi-node key-value store.\\nProblem Description\\nYou will build a distributed key-value store with three or more nodes. The key challenge is\\nto ensure that writes to the store are causally ordered. If event B is causally dependent on\\nevent A (e.g., a value is read and then updated), all nodes must process event A before they\\nprocess event B. Simple Lamport clocks are insufficient for this, so you will use Vector Clocks.\\nTechnology Constraints\\n• Programming Language: The entire application logic for the nodes and client must\\nbe written exclusively in Python.\\n• Containerization: The system must be containerized and orchestrated solely using\\nDocker and Docker Compose.\\nTasks\\nYour implementation should cover the following tasks:',\n",
       "  'metadata': {'creationdate': '2025-06-13T15:44:27+00:00',\n",
       "   'page': 1,\n",
       "   'doc_index': 5,\n",
       "   'creationDate': 'D:20250613154427Z',\n",
       "   'subject': '',\n",
       "   'author': '',\n",
       "   'source_file': 'Assignment 1.pdf',\n",
       "   'title': '',\n",
       "   'source': '../data/pdf/Assignment 1.pdf',\n",
       "   'format': 'PDF 1.5',\n",
       "   'file_path': '../data/pdf/Assignment 1.pdf',\n",
       "   'file_type': 'pdf',\n",
       "   'modDate': 'D:20250613154427Z',\n",
       "   'trapped': '',\n",
       "   'producer': 'pdfTeX-1.40.26',\n",
       "   'moddate': '2025-06-13T15:44:27+00:00',\n",
       "   'creator': 'LaTeX with hyperref',\n",
       "   'total_pages': 5,\n",
       "   'content_length': 985,\n",
       "   'keywords': ''},\n",
       "  'similarity_score': 0.014769375324249268,\n",
       "  'rank': 3}]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_retriever.retrieve(\"What do I have to do in Vector Clocks and Causal Ordering assignment?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e709b73",
   "metadata": {},
   "source": [
    "### Integration Vector db context pipeline with LLM output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f76a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple RAG pipeline with groq LLM\n",
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# 1. Initialize the ChatGroq LLM with the API key from environment variables\n",
    "chat_groq = ChatGroq(api_key=os.getenv(\"CHAT_GROQ_API_KEY\"), model_name=\"llama-3.1-8b-instant\" , temperature=0.1, max_tokens=4096)\n",
    "# chat_genai = ChatGoogleGenerativeAI(api_key=os.getenv(\"CHAT_GOOGLE_API_KEY\"), model=\"gemini-2.5-flash\", temperature=0.1, max_tokens=1024)\n",
    "\n",
    "# 2. Define a function to perform RAG with Groq LLM\n",
    "def rag_simple(query:str, retriever: RagRetriever = rag_retriever, llm: ChatGroq = chat_groq, top_k:int=5) -> str:\n",
    "    \"\"\"\n",
    "    Perform a simple RAG operation using the provided retriever and LLM.\n",
    "    \n",
    "    :param query: The input query string.\n",
    "    :param retriever: Instance of RagRetriever to fetch relevant documents.\n",
    "    :param llm: Instance of ChatGroq LLM to generate the answer.\n",
    "    :param top_k: Number of top relevant documents to retrieve.\n",
    "    :return: Generated answer from the LLM.\n",
    "    \"\"\"\n",
    "    # Step 1: Retrieve relevant documents\n",
    "    relevant_docs = retriever.retrieve(query, top_k=top_k)\n",
    "    \n",
    "    # Step 2: Prepare context for LLM\n",
    "    context = \"\\n\\n\".join([doc['content'] for doc in relevant_docs]) if relevant_docs else \"\"\n",
    "    if not context:\n",
    "        return \"No relevant documents found to answer the query.\"\n",
    "    \n",
    "    prompt = f\"Using the following context answer the question precisely Context:\\n\\n{context}\\n\\nQuestion: {query}\\nAnswer:\"\n",
    "    \n",
    "    # Step 3: Generate answer using LLM\n",
    "    response = llm.invoke([prompt.format(context=context, query=query)])\n",
    "    \n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a46ee662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta-llama/llama-guard-4-12b\n",
      "openai/gpt-oss-120b\n",
      "whisper-large-v3\n",
      "openai/gpt-oss-20b\n",
      "meta-llama/llama-4-maverick-17b-128e-instruct\n",
      "qwen/qwen3-32b\n",
      "moonshotai/kimi-k2-instruct-0905\n",
      "llama-3.3-70b-versatile\n",
      "llama-3.1-8b-instant\n",
      "groq/compound\n",
      "moonshotai/kimi-k2-instruct\n",
      "meta-llama/llama-prompt-guard-2-22m\n",
      "groq/compound-mini\n",
      "whisper-large-v3-turbo\n",
      "playai-tts-arabic\n",
      "meta-llama/llama-prompt-guard-2-86m\n",
      "allam-2-7b\n",
      "openai/gpt-oss-safeguard-20b\n",
      "playai-tts\n",
      "meta-llama/llama-4-scout-17b-16e-instruct\n"
     ]
    }
   ],
   "source": [
    "import groq\n",
    "client = groq.Groq(api_key=os.getenv(\"CHAT_GROQ_API_KEY\"))\n",
    "models = client.models.list()\n",
    "for m in models.data:\n",
    "    print(m.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "492e086d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: please provide the code for the assignment Dynamic Load Balancing for a Smart Grid?\n",
      "top K: 5, score threshold: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 29.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings for 1 texts with shape (1, 384)\n",
      "Retrieved 3 documents after applying score threshold.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I can provide a high-level design and some sample code for the Dynamic Load Balancing for a Smart Grid assignment. However, please note that this is a simplified example and may not cover all the requirements of the assignment.\n",
      "\n",
      "**System Design:**\n",
      "\n",
      "The system will consist of the following components:\n",
      "\n",
      "1. **EV Charging Simulator:** This component will simulate the Electric Vehicle (EV) charging requests and send them to the Load Balancer.\n",
      "2. **Load Balancer:** This component will receive the EV charging requests, determine the least loaded substation, and forward the request to that substation.\n",
      "3. **Substation:** This component will receive the EV charging requests from the Load Balancer, process them, and update the load status.\n",
      "4. **Monitoring Stack:** This component will collect and visualize key performance indicators (KPIs) such as load, request latency, and substation utilization.\n",
      "\n",
      "**Sample Code:**\n",
      "\n",
      "Here's a simplified example using Python and Flask for the Load Balancer and Substation components:\n",
      "\n",
      "**load_balancer.py**\n",
      "```python\n",
      "from flask import Flask, request\n",
      "from flask_cors import CORS\n",
      "import random\n",
      "\n",
      "app = Flask(__name__)\n",
      "CORS(app)\n",
      "\n",
      "# Simulate multiple substations\n",
      "substations = [\n",
      "    {\"id\": 1, \"load\": 0, \"capacity\": 100},\n",
      "    {\"id\": 2, \"load\": 0, \"capacity\": 100},\n",
      "    {\"id\": 3, \"load\": 0, \"capacity\": 100},\n",
      "]\n",
      "\n",
      "@app.route(\"/charge\", methods=[\"POST\"])\n",
      "def charge():\n",
      "    # Get the EV charging request\n",
      "    request_data = request.get_json()\n",
      "    ev_id = request_data[\"ev_id\"]\n",
      "    charge_amount = request_data[\"charge_amount\"]\n",
      "\n",
      "    # Determine the least loaded substation\n",
      "    least_loaded_substation = min(substations, key=lambda x: x[\"load\"])\n",
      "\n",
      "    # Forward the request to the least loaded substation\n",
      "    least_loaded_substation[\"load\"] += charge_amount\n",
      "    return {\"substation_id\": least_loaded_substation[\"id\"], \"load\": least_loaded_substation[\"load\"]}\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    app.run(debug=True)\n",
      "```\n",
      "\n",
      "**substation.py**\n",
      "```python\n",
      "from flask import Flask, request\n",
      "from flask_cors import CORS\n",
      "\n",
      "app = Flask(__name__)\n",
      "CORS(app)\n",
      "\n",
      "# Simulate a substation\n",
      "substation = {\"id\": 1, \"load\": 0, \"capacity\": 100}\n",
      "\n",
      "@app.route(\"/charge\", methods=[\"POST\"])\n",
      "def charge():\n",
      "    # Get the EV charging request\n",
      "    request_data = request.get_json()\n",
      "    ev_id = request_data[\"ev_id\"]\n",
      "    charge_amount = request_data[\"charge_amount\"]\n",
      "\n",
      "    # Process the request\n",
      "    substation[\"load\"] += charge_amount\n",
      "    return {\"substation_id\": substation[\"id\"], \"load\": substation[\"load\"]}\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    app.run(debug=True)\n",
      "```\n",
      "\n",
      "**Monitoring Stack:**\n",
      "\n",
      "For the monitoring stack, you can use a library like Prometheus and Grafana to collect and visualize KPIs such as load, request latency, and substation utilization.\n",
      "\n",
      "**Video Link:**\n",
      "\n",
      "To create a video demonstrating the project, you can use a tool like OBS Studio to record a screencast of the Load Balancer and Substation components in action. You can also add annotations and commentary to explain the system design and how it works.\n",
      "\n",
      "Note that this is a simplified example and you may need to modify it to fit the specific requirements of the assignment. Additionally, you may need to add more features such as authentication, authorization, and data persistence to make the system more robust.\n"
     ]
    }
   ],
   "source": [
    "answer = rag_simple(\"please provide the code for the assignment Dynamic Load Balancing for a Smart Grid?\", rag_retriever, chat_groq, top_k=5)\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
